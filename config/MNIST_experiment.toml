dataset = "MNIST"
device = "cuda"
seed = 88888888
epochs = 1000
batch_size=16
confactor='False'

[optimizer]

    opt = 'AdamW'
    lr = 0.005
    weight_decay=1e-05
    amsgrad='True'
    opt_scheduler='none'

[model]
    device = "cuda"

    [model.attention]
    L=1  # Attention model input nodes
    D=10  # Attention model intermediate nodes
    K=1    # Attention model output nodes

    [model.autoencoder]
#    type = 'conv'
    type = 'linear'
    batch_norm = 'True'
    n_features = 784
    hidden_neurons = [64, 32]
    hidden_activation = 'relu'
    dropout_rate = 0.1
