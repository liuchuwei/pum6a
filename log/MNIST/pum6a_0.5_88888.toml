seed = 88888

[dataload]

    dataset = "MNIST"
    num_bag = 500
    mean_nbag_length = 10
    var_nbag_length = 2
    confactor = 0.3
    target = 9
    seed = 88888

[model]

    model_chosen = 'pum6a' # pum6a, puma, iAE, puIF, RF, PU-SKC, puMIL, LSDD.toml, DSDD
    device = 'cuda'

    [model.feature_extractor]
#    type = 'conv'
#    hidden_neurons = [20, 50]
#    linear_input = 800
#    hidden_activation = 'relu'
#    kernal_size = 5
#    pool_kernal_size = 2
#    pool_stride = 2
    type = 'linear'
    n_features = 784
    dropout_rate = 0
    hidden_neurons = [512, 512] # 2
    batch_norm = false
    hidden_activation = 'relu'

    [model.classifier]
    input = 2

    [model.attention]
    L=512  # Attention model_factory input nodes
    D=256  # Attention model_factory intermediate nodes
    K=1  # Attention model_factory output nodes

[trainer]

    trainer_chosen='adanTrainer' # adanTrainer, puIF_Trainer, RF_Trainer, Trainer
    device = 'cuda'
    n_splits = 5    # 5-fold-cross-validataion
    save_dir = 'result/MNIST/pum6a/50'
    seed = 88888

    epochs = 300 # 100, 1000
    batch_size = 16
    freq = 0.5  # 50% freqency
    early_stopping = 3
    verbose = false

    [trainer.optimizer]

    opt = 'AdamW'
    lr = 0.001
    weight_decay=1e-05
    amsgrad=true
    opt_scheduler='none'